# 5-gm Анализ данных в разработке игр, 5 лабораторная работа
Отчет по лабораторной работе #5 выполнил(а):
- Шайдуров Савелий Сергеевич
- АТ-05

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

В данной работе коэффициент корреляции — это максимальное расстояние (1.42), допустимое от агента до цели, используемое для оценки результатов обучения. Логирование при таких значениях выглядит так:
![![distance3](![distance3](https://github.com/user-attachments/assets/8b349dc5-18f4-4d88-baf5-d851d1cab883)




Уменьшение максимального расстояния до 1 замедляет обучение агента: падает среднее вознаграждение, увеличивается отклонение от цели, и модель демонстрирует неэффективное “кружение” вокруг нее. Лог при дистанции 1:
![task 1](![distance3](https://github.com/user-attachments/assets/1eb4c055-86b4-4eb4-a984-50388d738257)

Если увеличить максимальную дистанцию до 3, то значение среднего вознаграждения резко возрастает и в течение всего обучения держится очень близко к 1, иногда даже равняется 1. При этом результаты обученной модели не очень хорошие: шар может не достигнуть куба, и при этом будет считаться, что он попал в цель, хотя визуально попадание не прошло, так как объеты были отдалены друг от друга. Показатель логирования при значении максимальной дистанции 1:



![task 1](![distance3 (1)](https://github.com/user-attachments/assets/09a2a3cb-5cff-4fbd-bfb4-67d16f9d2cf3)


Подводя итоги, можно сказать, что коэффициент коррелляции в данном случае будет влиять на то, насколько сильно ML-агент будет стремиться к своей цели и находить идеальную точку попадания, минимизируя среднее отклонение от нее.

Если значение близко к 1, то шар почти в 100% случаев достигает нужной цели. Это означает, что ML-агент обучился по опыту и будет выполнять данные функции. А чем меньше значение, тем хуже агент достигает своей цели, не получая награду за это.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

В качестве изменнения параметров файла, мною был выбран batch_size, то есть размер одного батча был изменен с 10 до 50. Это повлияло ,в большей степени, только на время обучения, которое уменьшилось почти в 2 раза:

![task 2](![batch50](https://github.com/user-attachments/assets/0b9f79d2-bd94-4c5d-9d85-1729369fa4e8)


Изменение количества эпох с 3 до 1 сократило время обучения и повысило среднее вознаграждение, при этом значения среднего отклонения остались непостоянными. Таким образом, хотя количество эпох обучения потенциально может влиять на финальную точность модели, в данном случае эффект не проявился, что объясняется изначальной нетребовательностью модели к большому количеству эпох.
![task 2](![epochs1](https://github.com/user-attachments/assets/94e20e3f-68d8-4466-9d83-b0a0acaf61fc)


Третий измененный параметр - это максимальное количество шагов при обучении. Обучение завершается в разы быстрее, за счет сокращения кол-ва шагов, но модель начинает работать в разы медленнее - при запуске ML-агент тратит время на поиск цели, а не совершает движение в сторону объекта. Логирование при таком параметре:

![task 2](![steps20000](https://github.com/user-attachments/assets/79559fe0-6aff-4c2c-9f56-43248127e77b)


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

ML-агент, разработанный в первом примере, демонстрирует применимость в сценариях с постоянно перемещающимися целями, такими как синглплеерные шутеры с преследующими NPC. Агент способен адаптироваться к изменению координат цели, что особенно полезно в 3D или изометрическом пространстве с физическими препятствиями. При этом, агент будет стремиться не только к поиску цели, но и к нахождению оптимального пути до нее.


ML-агент из второго примера хорошо подходит для пошаговых стратегий, таких как Hearthstone, где NPC должны выполнять повторяющиеся задачи, особенно в режимах, где есть элементы градостроительства и ресурсного менеджмента (Clash of Clans, Legends Mobile). В таких играх, если игрок меняет цель, агент легко перестроится и найдет новый путь, даже если окружающая среда постоянно меняется.

В заключение, использование ML-агентов наиболее оправдано в игровых мирах с динамически меняющимся окружением. В таких играх, как Minecraft или The Long Dark, где игрок имеет большую свободу передвижения, или где ландшафт меняется (например, строительство в Minecraft), ML-агенты могут эффективно ориентироваться и находить оптимальные маршруты к цели. Ручное управление поведением NPC в таких условиях было бы затруднительно из-за огромного количества переменных для учета.

## Выводы

В рамках данной лабораторной работы были обучены две различные модели ML-агентов: одна для поиска постоянно перемещающейся цели, а другая - для автоматизации добычи ресурсов NPC (перемещение между статическими точками). Для первой задачи были проведены несколько экспериментов с различными конфигурациями обучения, что позволило проанализировать влияние входных параметров на конечные результаты

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
